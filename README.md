This application is an application that reads data from a kafka topic and process it before storing it into a new topic named "user-login-processed"

1. How would you deploy this application in production?
    i. Infrastructure: Set up a production-grade infrastructure with sufficient resources (CPU, memory, storage) to handle the expected data load. Consider using cloud services like AWS, Azure, or Google Cloud for scalability and availability.
    ii. High Availability: Deploy Kafka and Zookeeper clusters in a highly available configuration to ensure data durability and fault tolerance. Use multiple Kafka brokers and Zookeeper nodes.

    iii. Monitoring: Implement monitoring and alerting solutions (e.g., Prometheus, Grafana) to track the health of Kafka clusters, consumer lag, resource usage, and system metrics. Set up alerts for critical issues.
    iv. Logging: Configure centralized logging (e.g., ELK stack) to collect and analyze logs generated by your application and Kafka.

    v. Security: Implement robust security measures, including SSL/TLS encryption, authentication, and authorization, to protect your Kafka clusters and data.
    vi. Deployment Automation: Use container orchestration platforms like Kubernetes for managing the application's containers and scaling up/down as needed.
    vii. Scaling: Ensure the Kafka consumer application can scale horizontally by adding more consumer instances as data volume increases.
    viii. Backup and Recovery: Implement backup and recovery strategies for your Kafka data, in case of data loss or cluster failures. For example, we can use Amazon MSK Connect Sink Connector to backup data into Amazon S3 and recover from it if we need it.
    
    iv. Deployment Strategies: Use deployment strategies like Canary deployments to minimize downtime during updates or changes to your application. This way, we can ensure the pipeline is stable and ready before scaling it up to fully productive.

2. What other components would you want to add to make this production-ready?
    i. Kafka Connect: Use Kafka Connect as a centralized data hub for simple data integration between things like databases, file systems, and more.
    ii. Schema Registry: Implement a schema registry for managing schemas to ensure data compatibility and data quality.
    iii. Load Balancers: Eventhough Kafka by default has a load balancer that implements Round-robin approach but as data volume increases we can switch to Parallelization.
    iv. Disaster Recovery: Develop a disaster recovery plan to handle catastrophic failures and data center outages. I will build a smaller secondary Kafka cluster,with separated brokers, zookeeper and disks, then use services liek MirrorMaker to fill it with data. That way, when the main kafka cluster goes down, we can switch to the secondary Kafka cluster while recovering the main one.
    v. Auto-Scaling: Implement auto-scaling mechanisms for Kafka clusters and consumer applications to handle varying workloads. We can utilize services like Amazon MSK.
    vi. Documentation: From my experience, I would always advice on making a clear and tharough documentations about deployment procedures, configurations, and troubleshooting guides. That way, when other engineer tries to take over the task, he doesn't have to go from the scratch to figure out what is going on, instead he could just read the documentation and figure out.

3. How can this application scale with a growing dataset?
    i. Scale Kafka Partitions: Increase the number of partitions for Kafka topics to allow for parallel processing. It allows more consumers to consume the messages simultaneously which helps in processing large volume of data.
    ii. Scale Consumer: Scale the Kafka consumers by adding more consumer as data grows. We can utilize container orchestration platforms like Kubernetes to implement this.
    iii. Optimizing Processing Logic: Optimize the data processing logic to be more efficient. For example, if process can go from O(N^2) to O(logN), that would be more than 100% sped up. The less steps it takes to arrive to end result means less processing needed  therefore higher speed.
    iv. Backpressure Handling: Implement backpressure handling mechanisms to prevent overwhelming the consumer application with data. This can involve rate limiting, buffering, or delaying messages.
    vi. Archiving Old Data: Implement data archiving and retention policies to move older data to long-term storage solutions (e.g., AWS S3) in order to keep the Kafka cluster manageable.
    vii. Capacity Planning: Base on data growth projection that may comes from different department, we can tharoughly review and adjust the kafka cluster capacity to prevent over-sizing or under-sizing.

*To run this application in your local machine, please follow steps below:*
1. Pull all the files (except myenv if you are not tring to run the python script separately) into your machine.
2. Install Docker
3. In your terminal, type "docker compose up --build" and it should start running
4. Double confirm by checking the terminal messages and see if the .py script has produced success messages.